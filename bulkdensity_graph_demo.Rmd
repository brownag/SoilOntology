---
title: "Bulk Density in the National Cooperative Soil Survey Characterization Database"
author: "Andrew G. Brown"
date: "10/29/2020" 
output: html_document
---
```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE)
```
 
## Bulk density is fundamental. 

From an ontological standpoint, bulk density is describing how the soil "body" occupies space, and therefore is essential to what the soil _is_, or at least how Humans experience the soil's physical (and chemical) state of being. Bulk density integrates many aspects of the soil fabric. Our measurement of bulk density is highly affected by the tools we use to extract a sample or measure the volume of a particular portion of soil material. It is also affected by whether (and to what degree) we consider water and/or rock fragments to be soil materials. This is discipline and region specific.

From an analytical standpoint, bulk density is a scaling factor applied to gravimetric measurements on soils to relate back to some reference condition. That is: relation of any measurement on a dry, sieved soil mass basis back to a volume of "real" soil (what we call a "Whole Soil" basis) requires a measurement, estimate or assumption about bulk density and coarse fragments.

In the realm of carbon stocks, for instance, bulk density is a property to consider carefully. Given measured organic carbon contents in, say, grams per kilogram across depth [soil materials change over depth] your choice as an analyst of the soil bulk density-depth function _matters_ to the estimate of the carbon stocks ($kg/m^2$) over a depth interval. 

This concept has practical application in the classification of the Humults in U.S. Soil Taxonomy.

**HB.** Other Ultisols that have _one or both_ of the following:

1. 0.9 percent (by weighted average) or more organic carbon in the upper 15 cm of the argillic or kandic horizon; or

2. $12 kg/m^2$ or more organic carbon between the mineral soil surface and a depth of 100 cm.
   
 - [_Humults_ criteria example](humults.html)

## Get the data

The `lab_analyte` table contains definitions of the analytes (physical and chemical measurements) used in the National Cooperative Soil Survey Characterization Database Snapshot. The records include various attributes defining schema and human-readable descriptions.

The delivery methods for these laboratory characterization data are a work in progress by the USDA-NRCS Soil Survey Staff. This effort would be greatly enhanced by collaboration with the Soil Ontology group. 

```{r}
library(soilDB)
library(ggplot2)
library(data.tree)
library(igraph)

methods <- SDA_query("SELECT DISTINCT procedure_key, proced_abbrev, requested_anal_name, 
                     proced_name, proced_code, proced_desc FROM lab_method_code")
# clean whitespace
methods$requested_anal_name <- trimws(methods$requested_anal_name)

# combine code + description
methods$code_desc <- sprintf("%s (%s)", methods$proced_code, methods$proced_desc)

# extract method group if available (Bulk Density, ....) <- before the comma
methods$method_group <- as.character(lapply(strsplit(methods$requested_anal_name, ",", TRUE), function(x) x[[1]]))

# build data.tree path: [root] -> [method group] -> [analysis name] -> [procedure name] -> [procedure code + (description)]
methods$path <- sprintf("%s|%s|%s|%s|%s", 'NCSS Lab Methods', 
                        methods$method_group, methods$requested_anal_name, 
                        methods$proced_name, methods$code_desc)

# init data.tree from path, attaching additional columns to leaves
res <- as.Node(methods, pathName = 'path', pathDelimiter = '|', 'procedure_key', 'proced_abbrev')

# demonstrate output for a single group of methods
res$`Bulk Density`

db_methods <- subset(methods, method_group == "Bulk Density")

# this is the brute force methods and not effective
# do.call('rbind', lapply(toupper(db_methods$analyte_name), function(x) {
#     print(x)
#     y <- stringdist(x, toupper(db_analytes$analyte_namerequested_anal_name), )
#     db_analytes[which(y == min(y))[[1]],]
#   }))

## salvage relationships by reverse engineering from physical/chemical properties tables
phys_names <- colnames(SDA_query("SELECT TOP(1) * from lab_physical_properties"))
idx <- grep("method$", phys_names)
phys_measure_names <- phys_names[idx-1]
phys_measure_methods <- phys_names[idx]
# nb: check  "estimated_om_plus_mineral"         "fiber_analysis_method"

# focus just on Db
db.idx <- grep("bulk_de|bd_", phys_measure_methods)
db_measur_cols <- phys_measure_names[db.idx]
db_method_cols <- phys_measure_methods[db.idx]

db_data_methods_table <- data.table::rbindlist(lapply(db_method_cols, function(x) {
  SDA_query(sprintf("SELECT %s FROM lab_physical_properties WHERE %s != ''", x, x))
}), fill = TRUE)
db_data_methods <- apply(as.data.frame(db_data_methods_table), 2, unique)

# lapply(db_data_methods, function(x) db_methods[na.omit(match(x, db_methods$proced_code)),])

analytes <- SDA_query("SELECT * from lab_analyte")
db_analytes <- subset(analytes, grepl("db_", analyte_abbrev))

# change some things to match tokens based on column names in database
db_analytes$analyte_name_clean <- gsub("1/3","third thirdbar",db_analytes$analyte_name)
db_analytes$analyte_name_clean <- gsub("<","lt",db_analytes$analyte_name_clean)
db_analytes$analyte_name_clean <- gsub("1/10","tenth tenthbar",db_analytes$analyte_name_clean)

db_methods$requested_anal_name_clean <- gsub("1/3","third thirdbar",db_methods$requested_anal_name)
db_methods$requested_anal_name_clean <- gsub("<","lt",db_methods$requested_anal_name_clean)
db_methods$requested_anal_name_clean <- gsub("1/10","tenth tenthbar",db_methods$requested_anal_name_clean)

tokens <- lapply(lapply(db_measur_cols, strsplit, "_"), function(x) x[[1]])
matching_analytes <- lapply(lapply(tokens, function(mtokens) {
  res <- lapply(mtokens, function(token) {
    grepl(token, db_analytes$analyte_name_clean, ignore.case = TRUE)
  })
  return(res)
}), function(x) {
  y <- rowSums(do.call('cbind', x))
  db_analytes[which(y == max(y)),]
})
best_analytes <- do.call('rbind', lapply(1:length(matching_analytes), function(z) { 
  matching_analytes[[z]]$name <- db_measur_cols[z] 
  matching_analytes[[z]]$aid <- 1:nrow(matching_analytes[[z]])
  matching_analytes[[z]]
}))

# TODO: abstract this beast
matching_methods <- lapply(lapply(tokens, function(mtokens) {
  res <- lapply(mtokens, function(token) {
    grepl(token, db_methods$requested_anal_name_clean, ignore.case = TRUE)
  })
  return(res)
}), function(x) {
  y <- rowSums(do.call('cbind', x))
  db_methods[which(y == max(y)),]
})
db_data_methods_num <- db_data_methods
names(db_data_methods_num) <- NULL
best_methods <- do.call('rbind', lapply(1:length(matching_methods), function(z) { 
  matching_methods[[z]] <- subset(matching_methods[[z]], proced_code %in% db_data_methods_num[[z]])
  if (nrow(matching_methods[[z]])) {
     matching_methods[[z]]$name <- db_measur_cols[z] 
     matching_methods[[z]]$mid <- 1:nrow(matching_methods[[z]])
  }
  matching_methods[[z]]
}))

# 25 different analytes (method classes) in NCSS characterization database contain prefix db_
# they all pertain to bulk density
nrow(db_analytes)

db_analytes2 <- unique(merge(best_analytes, best_methods)[,c("analyte_type", "aid", "analyte_abbrev", "mid", "proced_code")])
db_analytes_unpaired <- db_analytes[!(db_analytes$analyte_abbrev %in% db_analytes2$analyte_abbrev), c("analyte_type", "analyte_abbrev")]
db_analytes_unpaired$aid <- 1
db_analytes_unpaired$mid <- 1
db_analytes_unpaired$proced_code <- "No method code"
db_analytes_unpaired$proced_code[grepl("aggregate", db_analytes_unpaired$analyte_type)] <- "Not in snapshot?"
db_analytes2 <- rbind(db_analytes2, db_analytes_unpaired)
```

```{r, fig.width=10, fig.height=10, echo=FALSE}
set.seed(544)
db_analytes2$pathString <- paste("NCSSC_BulkDensity", 
                            db_analytes2$analyte_type, 
                            db_analytes2$proced_code,
                            db_analytes2$analyte_abbrev, 
                            sep = "/")
library(networkD3)
par(mar=c(0,0,0,0))
useRtreeList <- ToListExplicit(as.Node(db_analytes2), unname = TRUE)
radialNetwork(useRtreeList, fontSize = 20)
```

More important than the absolute differences _between_ methods is distinguishing which one was used (_if any!_) so that assumptions in derived values are made clear up front. Whether using measured values or otherwise, a person using data in a meta-analysis may want to be able to use their own assumptions, or at least validate our assumptions. A broader, more "comprehensive" set of method codes could uniquely identify major variants on SSL methods that may be encountered "in the wild." 

```{r, echo=FALSE}
knitr::kable(db_analytes[,c("analyte_name","analyte_abbrev","analyte_type")],
             caption = 'NCSSC Bulk Density Analyte Name, Abbreviations and Types', row.names = FALSE)
```

Bulk density _measurements_ are composited into what we call an _aggregate_ analyte. The `analyte_type` column distinguishes these records from records that are _derived_ from other analytes.

Each one of the aggregate analytes types may have records for several specific method codes. For the KSSL's use of these databases internally, this mechanism has been employed to accomodate both external methods, as well as obsolesence of internal methods. 

"Aggregate" means we make repeated measures on subsamples to assess precision and accuracy for individual samples (`labsampnum`, layers or horizons in a soil profile), and a summary function is applied to yield the final value. For a soil property as fundamental and variable as bulk density, this is a pretty important thing to do for quality control. The measurements are prone to error, so sometimes it is important to have a sanity check.

```{r}
# NB: fix the space v.s. underscore in database analyte_type == "derived_analyte" and dupe of aggregate analyte
db_analytes_agg <- subset(db_analytes, analyte_type == "aggregate analyte" | analyte_type == "aggregate_analyte")
analytes_der <- subset(analytes, analyte_type == "derived_analyte")
```

So, if bulk density is so important, how can we show this? The `analyte_algorithm` contains an expression that can be evaluated to calculate a _derived_ analyte from other analytes. We will search that field to find dependencies.

```{R}
# calculate derived analytes that depend on the aggregate analytes
db_analytes_agg_idx <- lapply(db_analytes_agg$analyte_abbrev, 
                              function(i) { grep(i, analytes_der$analyte_algorithm) })
names(db_analytes_agg_idx) <- db_analytes_agg$analyte_name
```

Most bulk density methods in use for soil science should fall into one of the following aggregate analyte "classes," where they will either conform with an existing SSL method code, or may warrant a new one:

```{r, echo=FALSE}
knitr::kable(data.frame(analyte_name = sort(db_analytes_agg$analyte_name)))
```

### Where are the bulk density _aggregate_ analyte codes used in _derived_ analyte calculations?

```{r, results='asis', echo=FALSE}
for (y in rev(seq_along(db_analytes_agg_idx))) {
   y.n <- names(db_analytes_agg_idx)[y]
   y.idx <- db_analytes_agg_idx[[y.n]]
   dat <- analytes_der[y.idx, "analyte_name"]
   if (length(dat) == 0) {
     dat <- data.frame(NA)
   } else { 
     dat <- as.data.frame(dat)
   }
   colnames(dat) <- "Derived Analyte"
   print(knitr::kable(dat, caption = y.n, row.names = FALSE, escape = FALSE))
}
```

OK, we see that most of these methods are not used as inputs to derived analytes, but the first two (_`r paste0(head( rev(names(db_analytes_agg_idx)), 2), collapse = ", ")`_) have many dependencies. Let's compare the data we have, before going down a rabbit hole on all of these methods. 

First, we build some comparable datasets; we take the "long format" table subsets queried from `lab_physical_properties ` and merge so we have records that are 1:1 with `labsampnum`. Query 0.33 kPa and oven-dry data, ignoring specific method codes within classes.

```{r}
# note there is a 100,000 record and 32 Mb JSON serialization limit with SDA_query()
db13b <- SDA_query("SELECT labsampnum, result_source_key, 
                           bulk_density_third_bar, bulk_density_third_bar_method
                    FROM lab_physical_properties 
                    WHERE bulk_density_third_bar_method != ''")

dbod <- SDA_query("SELECT labsampnum, result_source_key, 
                          bulk_density_oven_dry, bulk_density_oven_dry_method
                    FROM lab_physical_properties 
                    WHERE bulk_density_oven_dry_method != ''")
```

```{r}
# inner join based on common attributes (labsampnum [and result_source_key])
dbcb <- merge(db13b, dbod)
```

We find we have 6 methods associated with different types of 0.33 kPa bulk density. Two methods (`4A1d` and `DbWR1`) are used for 0.33 kPa; the latter methods (`4A1e`and `4A1f` and `NK`, and `4A1c`) are variants not defined in the most recent SSL methods manual. 

```{r, echo=FALSE}
table(dbcb$bulk_density_third_bar_method)
```

For oven-dry, we have `4A1h` and `DbWR1`.

```{r, echo=FALSE}
table(dbcb$bulk_density_oven_dry_method)
```

Most of the derived calculations use the Bulk Density at 1/3 bar (33 kPa), and some use oven dry. We will pull from the non-`DbWR1` methods first.

NOTE: Method code `4A1d` is obsolete code for _Bulk Density at 1/3 bar (33 kPa)_ moisture retention; now `3B1b.` `4A1h` is obsolete code for _Oven-dry Bulk Density_; now `3B1c`.

```{r}
dbcb_comparable1 <- subset(dbcb, bulk_density_third_bar_method %in% c("4A1d","3B1b") &
                                 bulk_density_oven_dry_method %in% c("4A1h","3B1c"))
dbcb_comparable1 <- dbcb_comparable1[complete.cases(dbcb_comparable1),]

table(dbcb_comparable1$bulk_density_third_bar_method)
table(dbcb_comparable1$bulk_density_oven_dry_method)
```

```{r, echo=FALSE}
ggplot(data = dbcb_comparable1, aes(x = bulk_density_third_bar, y = bulk_density_oven_dry)) + 
  geom_bin2d(bins = 50) + geom_abline(slope = 1, intercept = 0, colour = "red", lwd = 1) +
  labs(title = "Oven-dry (4A1h) versus 0.33 kPa Bulk Density (4A1d)",
       x = "Bulk Density, Oven-dry [g/cc]",
       y = "Bulk Density, 0.33 kPa moisture tension [g/cc]") +
  geom_text(aes(x = 2, y = 0.5, label = sprintf("n = %s (paired)", nrow(dbcb_comparable1)))) + 
  coord_fixed() +  expand_limits(x = c(0,3), y = c(0,3))

# increase in bulk density on oven-drying 
ggplot(data = dbcb_comparable1, aes(x = bulk_density_oven_dry - bulk_density_third_bar)) + 
  geom_density() +
  labs(title = "Increase in Bulk Density on Oven-drying (4A1h versus 4A1d)",    
       y = "Probability Density [-]",
       x = "Bulk Density, increase on drying [g/cc]") +
  geom_text(aes(x = 1, y = 1, label = sprintf("n = %s", nrow(dbcb_comparable1))))
```

Also, we will compare the `DbWR1` bulk density measures...

```{r}
dbcb_comparable2 <- subset(dbcb, bulk_density_third_bar_method == "DbWR1" &
                                 bulk_density_oven_dry_method == "DbWR1")
dbcb_comparable2 <- dbcb_comparable2[complete.cases(dbcb_comparable2),]

table(dbcb_comparable2$bulk_density_third_bar_method)
table(dbcb_comparable2$bulk_density_oven_dry_method)
```

```{r, echo=FALSE}
ggplot(data = dbcb_comparable2, aes(x = bulk_density_third_bar, y = bulk_density_oven_dry)) + 
  geom_bin2d(bins = 50) + geom_abline(slope = 1, intercept = 0, colour = "red", lwd = 1) +
  labs(title = "Oven-dry versus 0.33 kPa Bulk Density (DbWR1)",
       x = "Bulk Density, Oven-dry [g/cc]",
       y = "Bulk Density, 0.33 kPa moisture tension [g/cc]") + 
  geom_text(aes(x = 2, y = 1, label = sprintf("n = %s (paired)", nrow(dbcb_comparable2)))) + 
  coord_fixed() +  expand_limits(x = c(0, 5), y = c(0, 5))

# increase in bulk density on oven-drying 
ggplot(data = dbcb_comparable2, aes(x = bulk_density_oven_dry - bulk_density_third_bar)) + 
  geom_density() +
  labs(title = "Increase in Bulk Density on Oven-drying (DbWR1)",    
       y = "Probability Density [-]",
       x = "Bulk Density, increase on drying [g/cc]") +
  geom_text(aes(x = 1, y = 1, label = sprintf("n = %s", nrow(dbcb_comparable2))))
```

Finally, these mixed-method samples have `DbWR1` for their `third_bar` method and `4A1h` for their oven-dry.

```{r}
dbcb_mixed <- subset(dbcb, bulk_density_third_bar_method == "DbWR1" &
                           bulk_density_oven_dry_method == "4A1h")
table(dbcb_mixed$bulk_density_third_bar_method)
table(dbcb_mixed$bulk_density_oven_dry_method)
```

```{r, echo=FALSE}
ggplot(data = dbcb_mixed, aes(x = bulk_density_third_bar, y = bulk_density_oven_dry)) + 
  geom_bin2d(bins = 50) + geom_abline(slope = 1, intercept = 0, colour = "red", lwd = 1) +
  labs(title = "Oven-dry versus 0.33 kPa Bulk Density\n(mixed 4A1h oven dry; DbWR1 0.33 kPa)",
       x = "Bulk Density, Oven-dry [g/cc]",
       y = "Bulk Density, 0.33 kPa moisture tension [g/cc]") + 
  geom_text(aes(x = 2, y = 0.5, label = sprintf("n = %s (paired)", nrow(dbcb_mixed)))) + 
  coord_fixed() +  expand_limits(x = c(0, 3), y = c(0, 3))

# increase in bulk density on oven-drying 
ggplot(data = dbcb_mixed, aes(x = bulk_density_oven_dry - bulk_density_third_bar)) + 
  geom_density() +
  labs(title = "Increase in Bulk Density on Oven-drying\n(mixed 4A1h oven dry; DbWR1 0.33 kPa)",    
       y = "Probability Density [-]",
       x = "Bulk Density, increase on drying [g/cc]") +
  geom_text(aes(x = 0.5, y = 1, label = sprintf("n = %s", nrow(dbcb_mixed))))
```

Tabulate results to go with the graphs:

```{r}
# summary statistics reveal similar offsets and distribution of Db increase on oven drying
do_diff <- function(x) {
  difff <- with(x, bulk_density_oven_dry - bulk_density_third_bar)
  print(sprintf("Mean: %s", mean(difff)))
  print(quantile(difff, probs = c(0,0.01,0.05,0.5,0.95,0.99,1)))
}

do_diff(dbcb_comparable1)

do_diff(dbcb_comparable2)

do_diff(dbcb_mixed)
```

In total, we have `r nrow(dbcb_comparable1) + nrow(dbcb_comparable2)` measurements between the paired sets

```{r}
any(dbcb_comparable1$labsampnum %in% dbcb_comparable2$labsampnum)
nrow(dbcb_comparable1) + nrow(dbcb_comparable2) 
```

Moisture status and sample preparation matters internally for representation of bulk density. Knowing whether a sample was sieved and some estimate of the moisture state (field moist state, oven-dry, X tension) is essential to interpretation of bulk density values for soils. It is likely possible, with well-correlated data, to "convert" between different weight (moisture) bases, given other soil properties, but it is critical that primary data get allocated near as possible to their "correct" method to support this type of conversion. 

The inclusion of rock fragments (>2mm) in weight measures is a potentially very complicated aspect of bulk density, in that sometimes those fragments are appreciably affecting soil functions such as water movement or heat transfer, whereas they are not "soil material" per se, even if you managed to get the core in around them. So "should" they be included? Traditionally, the answer is no. You may also need to be making assumptions about particle density; 2.65 g/cc is a common figure for silicates.

If water or fragment content are not known, they are not known. There should be a way to annotate this. Ensuring representative volume is the main challenge, depending on your environment and sampling method. While there may not be much adherence to standard methods specifically as written in the SSL manual, effort should be made to categorize and provide options to support meta-correlation involving similar parameters. From a bare-minimum data quality standpoint users of the NCSS characterization  databases want to make sure they are not causing duplication by not filtering on method code; e.g. incorporating repeated measurements of bulk density on same/similar samples for different moisture states.
