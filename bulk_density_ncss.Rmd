---
title: "Bulk Density in the National Cooperative Soil Survey Characterization (NCSSC) Database and Beyond"
author: "Andrew G. Brown"
date: "`r Sys.Date()`"
output: 
  html_document:
    self_contained: FALSE
    mathjax: local
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
---
```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE, echo = FALSE)
```
 
## Soil Bulk Density

There is a great deal more information on the topics referenced in this document in the [National Soil Survey Handbook (NSSH)](https://www.nrcs.usda.gov/wps/portal/nrcs/detail/soils/ref/?cid=nrcs142p2_054242), 

Soil particle size distribution, mineralogy, grain packing, soil depth, and organic matter content are among the major properties affecting the bulk densities we observe in soil materials (NSSH, parts 618.32 "Rock fragments in the soil", 618.44 "Organic Matter", 618.45 "Parent Material, Kind, Modifier, and Origin")

Here are two links to graph diagrams showing relationships between analytes and also between bulk density methods specifically within the NCSSC database:

 - [NCSSC Database Analyte Graph](ncss_analyte_graph.html)
 
 - [NCSSC Aggregate to Derived Bulk Density and their Dependencies](ncss_bulk_density_derived_graph.html)

### Soil Materials

In the USDA system we have mineral and organic _soil materials_ (Keys to Soil Taxonomy, 12th edition, p. 3). 

_Mineral soil materials_ are particles passing a 2.0 mm sieve with carbon content (by weight) less than a threshold. The definition in  takes into account water saturation of the soil and clay content to the threshold. _Organic soil materials_ are defined as materials exceeding the carbon thresholds for mineral soil materials.

### The "Whole Soil" basis

The _Whole Soil_ is the basis we report many measurements on for NCSS products -- notably available water capacities, water retention values, and the like. But any measurement that is made on an oven-dry soil mass basis ultimately is corrected to the Whole Soil basis for broader use. This includes effects of larger rock/wood/non-soil materials that occupy volume but lack the bulk characteristics of soil materials. 

Soil materials are often the fundamental "object" of our study, but the content and distribution of larger fragment materials within the soil body is very important to soil classification and interpretations.

### Fine-earth Fraction and Soil Particle Size Distributions

The solid particles that comprise the Whole Soil are classified based on whether they pass a 2 mm sieve. This naturally provides distinction for materials that are granular in nature. The material that passes the sieve is the _fine-earth fraction_ and the material retained is the _coarse fraction_. In principle, a particle in a soil could be a mineral grain, an organic fiber, a microbe, a piece of gravel, a tree trunk, a boulder, etc.

Particles are single "pieces" of coherent/cemented material. The process of separating particle from one another systematically based on size and related physical characteristics is called _particle size analysis_. The relative abundance of [defined] particle size classes (ranges of sizes) in the _fine-earth fraction_ gives us the USDA textural classes (like "loam").

### Fundamental definitions

#### Particle density

We define the density of our broadly-defined "particle" $D_{p}$ as follows:

$$D_{p} = \frac{m_{particle}}{V_{particle}}$$

Assumptions are commonly used for particle density. A common $D_{p}$ figure for silicates is $2.65~g/cm^3$. In NASIS at the individual _Pedon_ and map unit _Component_ level we have fields and calculations for particle density. We also have the ability to measure it in the lab for soils or their separates.

#### Particle geometry

Aside from whether the particle passes a 2 mm sieve there are few limits on geometries that we observe in soil materials.

That said, there are often assumptions about particle geometry (e.g. spherical) for various analytical methods (e.g. Stokes' Law for pipette or hydrometer particle size analysis) and for expediency in physical calculations.

#### Bulk density

The "bulk" density of a sample integrates all of the masses of particles over a volume of Whole Soil. Conventionally, when coarse particles are present in that volume they are negated from the sample volume to calculate _soil material bulk density_.

$$D_{b,~soil~material} = \frac{m_{soil~material,~oven~dry}}{V_{soil~material}} = \frac{m_{soil~material,~oven~dry}}{V_{sample} - \frac{m_{fragments}}{D_{p,fragments}}}$$

When one considers that most soil analyses are reported on an oven-dry mass basis (e.g. $mg/kg$), bulk density is one of two critical scaling factors to relate to the "Whole Soil" reference condition. The second is the _coarse fraction_; usually rock fragments. 

The volume of fragments in the "layer" being summarized may be appreciably affecting functions such as water movement or heat transfer -- so they are important to recognize in the "Whole Soil" despite not being "soil materials." 

#### Whole Soil and Non-soil

Knowing where in the profile a bulk density sample came from, whether it was sieved, whether there were volumetric estimates of fragments, and the moisture state (e.g. field moist, oven-dry, X tension) is essential to interpretation of bulk density values for the Whole Soil body. 

If water or fragment contents for a sample or a parent morphologic/analytical "layer" are not known, they are not known. This makes it much harder to interpret bulk density "data" (which is always a calculation involving several measurements or estimates) at face value -- you do not know what materials the mass refers to. 

Efforts should be made to categorize data sets and provide options to support meta-correlation involving _similar_ parameters when constituent measurements for bulk density are missing or unreliable. An example related to numeric _coarse fragment volume_ would be _textural class modifiers_ (terms like "very gravelly") applied to USDA textural classes.

### Rock fragment volume estimates

See NSSH part 618.32 "Fragments in the Soil"

For an individual sample, we will get sieve separates for the coarse fraction from the lab, but we really lean on the field observations of these properties. Mass basis measurements of fragments (accurate) are be converted to _meaningful_ volume estimates by dividing by $D_p$. With a standard sieve and a rugged scale, one can relatively readily do field measurements of fragments up to 76 mm.

For larger fragments, if a large proportion are near or greater than 76mm, you would need to "sieve" at least several hundred kilograms of sample to get a "representative" weight per volume -- so most commonly this is done by visual estimate (i.e. as part of the routine profile description).

 - The 2 - 20 mm and 20 - 76 mm fractions are sieved and weighed in the field at sampling time. The field measurements and estimations are provided to the lab as metadata, ultimately paired by `labsampnum`. 
 
 - The larger fractions are stored in the NASIS _Pedon_ object. _Pedon Horizon Fragments_ holds weight or volume $\%$ measures as needed; volume $\%$ is what is conventionally reported in our maps.

## We calculate organic carbon stocks with bulk density

A Carbon pool, or stock, is a value with units of mass C per area. It is an excellent applied example of the effects of soil bulk density on the types of measurements we make in soils and show in soil databases.

In USDA Soil Taxonomy, we typically are dealing in units/scale of  $kg/m^2$; where the 1-D definition is aggregation soil length over $n$ layers; so "stocks" are conventionally reported over a specific interval (e.g. 0 to 1 m).

$$C{p,soil~material} = \sum_{i=1}^{n}\left(\frac{C_i}{100} * D_{b, i} * L_{i} * \left(1 - \frac{V_{fragments, i}}{V_{Whole~Soil, i}}\right)\right)$$

$C_p$ is the sum of organic carbon contents $C_i$ ($\%~by~mass$) multiplied by bulk density $D_{b, i}$ ($kg/m^3$) multiplied by layer thickness $L_i$ ($m$). These can be scaled based on the ratio of the layer volume occupied by fragments to the total volume of the layer: $\left(1 - \frac{V_{fragments, i}}{V_{Whole~Soil, i}}\right)$. 

NOTE: The most appropriate "total fragment volume" (accounting for all size classes) for a layer often is not the same as what was observed in an individual sieved core or clod sample at the lab. For instance, the lab reports the sieve separates for 2 - 20 mm and 20 - 76 mm fractions when present, but cannot give you a reliable estimate of any of the rock fragment volumes in the field. Knowing these values precisely relies on the field weight measurements made during sampling.

The percentage cobbles, stones or boulders (using USDA fragment terms) need to come from the visual estimates in all but the most extensively characterized cases (where all size classes are weighed over a _large_ volume of soil material). The precision of visual estimates is not as "quantiative" as the portions measured using scales, but is far more prevalent. 

## Analyte and Method Metadata Wrangling

There is more to this than oven-dry bulk density and sieving fragments, though.

The `lab_analyte` table is found in Soil Data Access (SDA), as well as the various lab data snapshots. SDA is the web API for accessing most of the public data available from the NCSS. This table contains definitions of the analytes (physical and chemical measurements) used in the National Cooperative Soil Survey Characterization Database Snapshot.

The `lab_method_code` table indexes the specific procedures used to measure analytes, abbreviation codes, and references. There is not a 1:1 relationship between analytes and methods. In some cases, older methods have become obsolete and replaced with new method codes that measure the same analyte, so there are many method codes for one analyte. Some methods measure multiple analytes. Some analytes are more specific and have only a single method code in use. Further, there are derivative analytes that are based on calculations using other analytes.

There is a significant amount of information and metadata stored in the collective group of databases that feed into NASIS (the National Soil Information System).

In NASIS, there are still some important linkages to various internal databases held together by dedicated software and manual interventions. Recognizing important linkages within the database currently requires some insider knowledge. It would be good to make some of these linkages more explicit, transparent and portable. 

New delivery methods for the laboratory characterization data are a work in progress by the USDA-NRCS Soil Survey Staff. This effort would be greatly enhanced by collaboration with the Soil Ontology group. 

```{r, fig.width=10, fig.height=10, echo=FALSE}
library(soilDB)
library(ggplot2)
library(data.tree)

methods <- SDA_query("SELECT DISTINCT procedure_key, proced_abbrev, requested_anal_name, 
                     proced_name, proced_code, proced_desc FROM lab_method_code")
# clean whitespace
methods$requested_anal_name <- trimws(methods$requested_anal_name)

# extract method group if available (Bulk Density, ....) <- before the comma
methods$method_group <- as.character(lapply(strsplit(methods$requested_anal_name, ",", TRUE), function(x) x[[1]]))

# build data.tree path: [root] -> [method group] -> [analysis name] -> [procedure name] -> [procedure code + (description)]
methods$path <- sprintf("%s|%s|%s", 'NCSS Lab Methods',  methods$method_group, methods$proced_code)

# init data.tree from path, attaching additional columns to leaves
res <- as.Node(methods, pathName = 'path', pathDelimiter = '|')
# plot(res, output = "visNetwork")

## salvage relationships by reverse engineering from physical/chemical properties tables
phys_names <- colnames(SDA_query("SELECT TOP(1) * from lab_physical_properties"))
idx <- grep("method$", phys_names)
phys_measure_names <- phys_names[idx - 1]
phys_measure_methods <- phys_names[idx]
# nb: check  "estimated_om_plus_mineral"         "fiber_analysis_method"

# focus just on Db
db.idx <- grep("bulk_de|bd_", phys_measure_methods)
db_measur_cols <- phys_measure_names[db.idx]
db_method_cols <- phys_measure_methods[db.idx]

if (!file.exists("db_data_methods_table.rda")) {
  # identify method codes referenced for each "data" column
  db_data_methods_table <- data.table::rbindlist(lapply(db_method_cols, function(x) {
    SDA_query(sprintf("SELECT %s FROM lab_physical_properties WHERE %s != ''", x, x))
  }), fill = TRUE)
  save(db_data_methods_table, file = "db_data_methods_table.rda")
} else {
  load("db_data_methods_table.rda")
}
db_data_methods <- apply(as.data.frame(db_data_methods_table), 2, unique)

# lapply(db_data_methods, function(x) db_methods[na.omit(match(x, db_methods$proced_code)),])

# get the analyte table
analytes <- SDA_query("SELECT * from lab_analyte")

# filter for bulk density related analytes
db_analytes <- subset(analytes, grepl("db_", analyte_abbrev))

# change some things to match tokens based on column names in database
db_analytes$analyte_name_clean <- gsub("Bulk Density","Bulk Density Db",db_analytes$analyte_name)
db_analytes$analyte_name_clean <- gsub("1/3","third thirdbar",db_analytes$analyte_name)
db_analytes$analyte_name_clean <- gsub("<","lt",db_analytes$analyte_name_clean)
db_analytes$analyte_name_clean <- gsub("1/10","tenth tenthbar",db_analytes$analyte_name_clean)

db_methods <- subset(methods, method_group == "Bulk Density")
db_methods$requested_anal_name_clean <- gsub("1/3","third thirdbar",db_methods$requested_anal_name)
db_methods$requested_anal_name_clean <- gsub("<","lt",db_methods$requested_anal_name_clean)
db_methods$requested_anal_name_clean <- gsub("1/10","tenth tenthbar",db_methods$requested_anal_name_clean)

# the DbWR1 method code is used for multiple moisture states
moisture_states <- db_methods[c(1:2,1:2),]
moisture_states$requested_anal_name_clean <- paste0(moisture_states$requested_anal_name_clean, rep(c(" third thirdbar", 
                                                                                                     " oven dry")))
db_methods <- rbind(moisture_states, db_methods[2:nrow(db_methods),])

tokens <- lapply(lapply(db_measur_cols, strsplit, "_"), function(x) x[[1]])
matching_analytes <- lapply(lapply(tokens, function(mtokens) {
    res <- lapply(mtokens, function(token) {
      grepl(token, db_analytes$analyte_name_clean, ignore.case = TRUE)
    })
    return(res)
  }), function(x) {
    y <- rowSums(do.call('cbind', x))
    db_analytes[which(y == max(y)),]
  })
good_analytes <- do.call('rbind', lapply(1:length(matching_analytes), function(z) { 
    if(nrow(matching_analytes[[z]])) {
      matching_analytes[[z]]$name <- db_measur_cols[z] 
      matching_analytes[[z]]$aid <- 1:nrow(matching_analytes[[z]])
      matching_analytes[[z]]
    }
  }))
# take first and most specific column name from analyte table only
best_analytes <- do.call('rbind', lapply(split(good_analytes, good_analytes$analyte_name_clean), function(x) {
    return(x[which(nchar(x$name) == max(nchar(x$name)))[1],])
  }))

# TODO: abstract this beast
matching_methods <- lapply(lapply(tokens, function(mtokens) {
  res <- lapply(mtokens, function(token) {
    grepl(token, db_methods$requested_anal_name_clean, ignore.case = TRUE)
  })
  return(res)
}), function(x) {
  y <- rowSums(do.call('cbind', x))
  db_methods[which(y == max(y)),]
})
db_data_methods_num <- db_data_methods
names(db_data_methods_num) <- NULL
best_methods <- do.call('rbind', lapply(1:length(matching_methods), function(z) { 
  matching_methods[[z]] <- subset(matching_methods[[z]], proced_code %in% db_data_methods_num[[z]])
  if (nrow(matching_methods[[z]])) {
     matching_methods[[z]]$name <- db_measur_cols[z] 
     matching_methods[[z]]$mid <- 1:nrow(matching_methods[[z]])
     matching_methods[[z]]
  }
}))

# 25 different analytes (method classes) in NCSS characterization database contain prefix db_
# they all pertain to bulk density
# nrow(db_analytes)

db_analytes2 <- merge(best_analytes, best_methods)[,c("analyte_type", "aid", "analyte_abbrev", "mid", "proced_code")]
db_analytes_unpaired <- db_analytes[!(db_analytes$analyte_abbrev %in% db_analytes2$analyte_abbrev), c("analyte_type", "analyte_abbrev")]
db_analytes_unpaired$aid <- 1
db_analytes_unpaired$mid <- 1
db_analytes_unpaired$proced_code <- "No method code"
db_analytes_unpaired$proced_code[grepl("aggregate", db_analytes_unpaired$analyte_type)] <- "Not in snapshot?"
db_analytes2 <- rbind(db_analytes2, db_analytes_unpaired)
```

Assumptions in derived values need to be made clear up front. Some commonly used values relating to saturation, "field capacity" and 15 bar moisture are actually _derived_ from other measurements. 

### Correlation and pedotransfer functions

It is possible with well-correlated data to estimate conversions between different bases, given other soil properties. This is something we do routinely in NASIS. If this is a goal, it is absolutely critical that primary data with the relevant metadata get allocated _near as possible_ to their "correct" method. 

Whether using measured values or otherwise, a person using data in a meta-analysis may want to be able to use their own assumptions, or at least validate our assumptions. A broader, more "comprehensive" set of method codes could uniquely identify major variants of methods that may be encountered "in the wild" to further enhance interoperability.

```{r, echo=FALSE}
knitr::kable(db_analytes[,c("analyte_name","analyte_abbrev","analyte_type")],
             caption = 'NCSSC Bulk Density Analyte Name, Abbreviations and Types', row.names = FALSE)
```

Bulk density _measurements_ are composited into what we call an _aggregate_ analyte. The `analyte_type` column distinguishes these records from records that are _derived_ from other analytes.

Each one of the aggregate analytes types may have records for several specific method codes. For the KSSL's use of these databases internally, this mechanism has been employed to accomodate both external methods, as well as obsolesence of internal methods. 

From a bare-minimum data quality standpoint users of the NCSS Characterization database want to make sure they are not causing duplication by method codes that should be mutually exclusive; e.g. combining measurements of oven dry bulk density with 1/3 bar for same `labsampnum`.

"Aggregate" means we make repeated measures on subsamples to assess precision and accuracy for individual samples (`labsampnum`, layers or horizons in a soil profile), and a summary function is applied to yield the final value. For a soil property as fundamental and variable as bulk density, this is a pretty important thing to do for quality control. The measurements are prone to error, so sometimes it is important to have a sanity check.

```{r}
# NB: fix the space v.s. underscore in database analyte_type == "derived_analyte" and dupe of aggregate analyte
db_analytes_agg <- subset(db_analytes, analyte_type == "aggregate analyte" | analyte_type == "aggregate_analyte")
analytes_der <- subset(analytes, analyte_type == "derived_analyte")
```

So, if bulk density is so important, how can we show this? The `analyte_algorithm` contains an expression that can be evaluated to calculate a _derived_ analyte from other analytes. We will search that field to find dependendent values.

```{R}
# calculate derived analytes that depend on the aggregate analytes
db_analytes_agg_idx <- lapply(db_analytes_agg$analyte_abbrev, 
                              function(i) { grep(i, analytes_der$analyte_algorithm) })
names(db_analytes_agg_idx) <- db_analytes_agg$analyte_name
```

Most bulk density methods in use for soil science should fall into one of the following aggregate analyte "classes," where they will either conform with an existing SSL method code, or may warrant a new one:

```{r, echo=FALSE}
knitr::kable(data.frame(analyte_name = sort(db_analytes_agg$analyte_name)))
```

## Where are the bulk density _aggregate_ analyte codes used in _derived_ analyte calculations?

```{r, results='asis', echo=FALSE}
for (y in rev(seq_along(db_analytes_agg_idx))) {
   y.n <- names(db_analytes_agg_idx)[y]
   y.idx <- db_analytes_agg_idx[[y.n]]
   dat <- analytes_der[y.idx, "analyte_name"]
   if (length(dat) == 0) {
     dat <- data.frame(NA)
   } else { 
     dat <- as.data.frame(dat)
   }
   colnames(dat) <- "Derived Analyte"
   print(knitr::kable(dat, caption = y.n, row.names = FALSE, escape = FALSE))
}
```

OK, we see that most of these methods are not used as inputs to derived analytes, but the first two (_`r paste0(head( rev(names(db_analytes_agg_idx)), 2), collapse = ", ")`_) have many dependents. 

Let's compare the data we have, before going down a rabbit hole on all of these different methods. 

First, we build some comparable datasets; we take the "long format" table subsets queried from `lab_physical_properties` and merge so we have records that are 1:1 with `labsampnum`. Query 0.33 kPa and oven-dry data, ignoring specific method codes within classes.

```{r, echo=TRUE}
if (!file.exists("sda_db_common.rda")) {
  # note there is a 100,000 record and 32 Mb JSON serialization limit with SDA_query()
  db13b <- SDA_query("SELECT labsampnum, result_source_key, 
                             bulk_density_third_bar, bulk_density_third_bar_method
                      FROM lab_physical_properties 
                      WHERE bulk_density_third_bar_method != ''")
  
  dbod <- SDA_query("SELECT labsampnum, result_source_key, 
                            bulk_density_oven_dry, bulk_density_oven_dry_method
                      FROM lab_physical_properties 
                      WHERE bulk_density_oven_dry_method != ''")
  
  save(db13b, dbod, file = "sda_db_common.rda")
} else {
  load("sda_db_common.rda")
}
```

```{r}
# inner join based on common attributes (labsampnum [and result_source_key])
dbcb <- merge(db13b, dbod)
```


We find we have 6 methods associated with different types of 0.33 kPa bulk density. Two methods (`4A1d` and `DbWR1`) are used for 0.33 kPa; the latter methods (`4A1e`and `4A1f` and `NK`, and `4A1c`) are variants not defined in the most recent SSL methods manual. 

```{r, echo=FALSE}
knitr::kable(table(dbcb$bulk_density_third_bar_method), col.names = c("Method","Count"), caption = "Third Bar Method")
```

For oven-dry, we have `4A1h` and `DbWR1`.

```{r, echo=FALSE}
knitr::kable(table(dbcb$bulk_density_oven_dry_method), col.names = c("Method","Count"), caption = "Oven Dry Method")
```

Most of the derived calculations use the Bulk Density at 1/3 bar (33 kPa), and some use oven dry. We will pull from the non-`DbWR1` methods first.

NOTE: Method code `4A1d` is obsolete code for _Bulk Density at 1/3 bar (33 kPa)_ moisture retention; now `3B1b.` `4A1h` is obsolete code for _Oven-dry Bulk Density_; now `3B1c`.

```{r}
# summary statistics reveal similar offsets and distribution of Db increase on oven drying
do_diff <- function(x) {
  difff <- with(x, bulk_density_oven_dry - bulk_density_third_bar)
  knitr::kable(data.frame(Q = t(quantile(difff, probs = c(0,0.01,0.05,0.5,0.95,0.99,1))), mean(difff), sd(difff)),
                     col.names = c("Q0", "Q1", "Q5", "Q50", "Q95", "Q99", "Q100", "MEAN", "SD"), digits = 2)
}
```

```{r}
dbcb_comparable1 <- subset(dbcb, bulk_density_third_bar_method %in% c("4A1d","3B1b") &
                                 bulk_density_oven_dry_method  %in% c("4A1h","3B1c"))
dbcb_comparable1 <- dbcb_comparable1[complete.cases(dbcb_comparable1),]

knitr::kable(table(dbcb_comparable1$bulk_density_third_bar_method), col.names = c("Method","Count"), caption = "Third Bar Method")
knitr::kable(table(dbcb_comparable1$bulk_density_oven_dry_method), col.names = c("Method","Count"), caption = "Oven Dry Method")
```

```{r, echo=FALSE}
ggplot(data = dbcb_comparable1, aes(x = bulk_density_third_bar, y = bulk_density_oven_dry)) + 
  geom_bin2d(bins = 50) + geom_abline(slope = 1, intercept = 0, colour = "red", lwd = 1) +
  labs(title = "Oven-dry (4A1h) versus 0.33 kPa Bulk Density (4A1d)",
       x = "Bulk Density, Oven-dry [g/cc]",
       y = "Bulk Density, 0.33 kPa moisture tension [g/cc]") +
  geom_text(aes(x = 2, y = 0.5, label = sprintf("n = %s (paired)", nrow(dbcb_comparable1)))) + 
  coord_fixed() +  expand_limits(x = c(0,3), y = c(0,3))

# increase in bulk density on oven-drying 
ggplot(data = dbcb_comparable1, aes(x = bulk_density_oven_dry - bulk_density_third_bar)) + 
  geom_density() +
  labs(title = "Increase in Bulk Density on Oven-drying (4A1h versus 4A1d)",    
       y = "Probability Density [-]",
       x = "Bulk Density, increase on drying [g/cc]") +
  geom_text(aes(x = 1, y = 1, label = sprintf("n = %s", nrow(dbcb_comparable1))))
```

```{r, results='asis'}
do_diff(dbcb_comparable1)
```

Also, we will compare the `DbWR1` bulk density measures...

```{r}
dbcb_comparable2 <- subset(dbcb, bulk_density_third_bar_method == "DbWR1" &
                                 bulk_density_oven_dry_method == "DbWR1")
dbcb_comparable2 <- dbcb_comparable2[complete.cases(dbcb_comparable2),]

knitr::kable(table(dbcb_comparable2$bulk_density_third_bar_method), col.names = c("Method","Count"), caption = "Third Bar Method")
knitr::kable(table(dbcb_comparable2$bulk_density_oven_dry_method), col.names = c("Method","Count"), caption = "Oven Dry Method")
```

```{r, echo=FALSE}
ggplot(data = dbcb_comparable2, aes(x = bulk_density_third_bar, y = bulk_density_oven_dry)) + 
  geom_bin2d(bins = 50) + geom_abline(slope = 1, intercept = 0, colour = "red", lwd = 1) +
  labs(title = "Oven-dry versus 0.33 kPa Bulk Density (DbWR1)",
       x = "Bulk Density, Oven-dry [g/cc]",
       y = "Bulk Density, 0.33 kPa moisture tension [g/cc]") + 
  geom_text(aes(x = 2, y = 1, label = sprintf("n = %s (paired)", nrow(dbcb_comparable2)))) + 
  coord_fixed() +  expand_limits(x = c(0, 5), y = c(0, 5))

# increase in bulk density on oven-drying 
ggplot(data = dbcb_comparable2, aes(x = bulk_density_oven_dry - bulk_density_third_bar)) + 
  geom_density() +
  labs(title = "Increase in Bulk Density on Oven-drying (DbWR1)",    
       y = "Probability Density [-]",
       x = "Bulk Density, increase on drying [g/cc]") +
  geom_text(aes(x = 1, y = 1, label = sprintf("n = %s", nrow(dbcb_comparable2))))
```

```{r, results='asis'}
do_diff(dbcb_comparable2)
```

In total, we have `r nrow(dbcb_comparable1) + nrow(dbcb_comparable2)` measurements between the "paired" sets

```{r, echo=TRUE}
any(dbcb_comparable1$labsampnum %in% dbcb_comparable2$labsampnum)
```

Finally, these mixed-method samples have `DbWR1` for their `third_bar` method and `4A1h` for their oven-dry.

```{r}
dbcb_mixed <- subset(dbcb, bulk_density_third_bar_method == "DbWR1" &
                           bulk_density_oven_dry_method == "4A1h")
knitr::kable(table(dbcb_mixed$bulk_density_third_bar_method), col.names = c("Method","Count"), caption = "Third Bar Method")
knitr::kable(table(dbcb_mixed$bulk_density_oven_dry_method), col.names = c("Method","Count"), caption = "Oven Dry Method")
```

```{r, echo=FALSE}
ggplot(data = dbcb_mixed, aes(x = bulk_density_third_bar, y = bulk_density_oven_dry)) + 
  geom_bin2d(bins = 50) + geom_abline(slope = 1, intercept = 0, colour = "red", lwd = 1) +
  labs(title = "Oven-dry versus 0.33 kPa Bulk Density\n(mixed 4A1h oven dry; DbWR1 0.33 kPa)",
       x = "Bulk Density, Oven-dry [g/cc]",
       y = "Bulk Density, 0.33 kPa moisture tension [g/cc]") + 
  geom_text(aes(x = 2, y = 0.5, label = sprintf("n = %s (paired)", nrow(dbcb_mixed)))) + 
  coord_fixed() +  expand_limits(x = c(0, 3), y = c(0, 3))

# increase in bulk density on oven-drying 
ggplot(data = dbcb_mixed, aes(x = bulk_density_oven_dry - bulk_density_third_bar)) + 
  geom_density() +
  labs(title = "Increase in Bulk Density on Oven-drying\n(mixed 4A1h oven dry; DbWR1 0.33 kPa)",    
       y = "Probability Density [-]",
       x = "Bulk Density, increase on drying [g/cc]") +
  geom_text(aes(x = 0.5, y = 1, label = sprintf("n = %s", nrow(dbcb_mixed))))
```

```{r, results='asis'}
do_diff(dbcb_mixed)
```

## Conclusion

Bulk density integrates many aspects of the soil fabric. Bulk density is describing how the soil "body" occupies space. Our measurement of bulk density is highly affected by the tools we use to extract a sample or measure the volume of a particular portion of soil material. It is also affected by whether (and to what degree) we consider water and/or rock fragments to be soil materials. This is discipline and region specific, but within the National Cooperative Soil Survey we have a robust system for describing these concepts.

## Applied Example

In the realm of carbon stocks, for instance, bulk density is a property to consider carefully. Given measured organic carbon contents in, say, grams per kilogram across depth [soil materials change over depth] your choice as an analyst of the soil bulk density-depth function _matters_ to the estimate of the carbon stocks ($kg/m^2$) over a depth interval. 

The relevance of bulk density to carbon stocks can be seen in the classification of the Humults in U.S. Soil Taxonomy.

**Here is an excerpt from the Ultisols suborder key:**

**HB.** Other Ultisols that have _one or both_ of the following:

1. 0.9 percent (by weighted average) or more organic carbon in the upper 15 cm of the argillic or kandic horizon; or

2. $12 kg/m^2$ or more organic carbon between the mineral soil surface and a depth of 100 cm.'

You will notice that criterion #1 is a carbon mass-percent weighted average, and criterion #2 is a carbon stock. See the Humults criteria demo for an exploration of soil carbon related taxonomic criteria in the _Humults_
   
 - [_Humults_ criteria example](humults.html)

## References

U.S. Department of Agriculture, Natural Resources Conservation Service. National Soil Survey Handbook (NSSH), title 430-VI. <http://www.nrcs.usda.gov/wps/portal/nrcs/detail/soils/ref/?cid=nrcs142p2_054242> (accessed 06 November 2020).

Soil Survey Staff. 2014. Keys to Soil Taxonomy, 12th ed. USDA-Natural Resources Conservation Service, Washington, DC.
<https://www.nrcs.usda.gov/wps/portal/nrcs/detail/soils/survey/class/taxonomy/?cid=nrcs142p2_053580>.
